# Feature Scaling 
## StandardScaler() to normalize the features
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x_train=scaler.fit_transform(train[['Var1','Var2']])
x_test=scaler.fit_transform(test[['Var1','Var2']])
----------------------------------------------------------------------------------------
# split into train & test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  

----------------------------------------------------------------------------------------
# Linear Regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model = model.fit(x, y)
----------------------------------------------------------------------------------------
# Logistic Regression
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(x_train, y_train)
----------------------------------------------------------------------------------------
# Train KNN and prediction
from sklearn.neighbors import KNeighborsClassifier  
model = KNeighborsClassifier(n_neighbors=5)  # 5 is the common choice
model.fit(X_train, y_train)  

## The mean of error for all the predicted values where K ranges from 1 and 40.
error = []
## Calculating error for K values between 1 and 40
for i in range(1, 40):  
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != y_test))
    
## plot the error values against K values
plt.figure(figsize=(12, 6))  
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')  
plt.xlabel('K Value')  
plt.ylabel('Mean Error')  
----------------------------------------------------------------------------------------
# Decision Trees
from sklearn.tree import DecisionTreeClassifier
classifier=DecisionTreeClassifier()
classifier.fit(X_train,y_train)
y_pred=classifier.predict(X_test)

## Visualize the tree
from sklearn.externals.six import StringIO  
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus

dot_data = StringIO()
export_graphviz(classifier, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

from sklearn.tree import DecisionTreeRegressor
regressor=DecisionTreeRegressor()
regressor.fit(X_train,y_train)
y_pred=regressor.predict(X_test)
----------------------------------------------------------------------------------------
# Random Forest
from sklearn.ensemble import RandomForestRegressor
# Instantiate model with 1k decision trees
rf=RandomForestRegressor(n_estimators=1000,random_state=42)
rf.fit(features_train,labels_train)
labels_pred=rf.predict(features_test)

# Visualize
# limit the depth of trees in the forest to produce an understandable image.
# Instantiate model with 10 decision trees & Limit depth of tree to 3 levels

import os     
os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'

from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus
from sklearn.tree import export_graphviz
import pydot

# Visualizing a Single Decision Tree
# pull 1 tree from the forest
tree=rf.estimators_[5]
dot_data = StringIO()
export_graphviz(tree, out_file=dot_data,feature_names=features_list,filled=True, rounded=True,
                special_characters=True,precision=1)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

# Variable Importance
# How much can a variable improve predictions (Only numerical features)
importances=list(rf.feature_importances_)
feature_importances=[(feature,round(importance,2)) for feature, importance in zip(features_list,importances)]
# sort by most important first
feature_importances=sorted(feature_importances,key=lambda x:x[1], reverse=True)
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

# Visualize those importance features
# list of x locations for plotting
x_values = list(range(len(importances)))
plt.bar(x_values, importances, orientation = 'vertical')
plt.xticks(x_values, features_list, rotation='vertical')
plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');
----------------------------------------------------------------------------------------



y_predict = model.predict(test["Scores"])
y_predict

# Model validation
model.score(x,y)

# Classification report + Confusion matrix
from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test, y_pred))  
print(classification_report(y_test, y_pred))  

# MAE, MSE
from sklearn import metrics  
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) 

print('Mean Squared Error 1:', metrics.mean_squared_error(y_train, y_pred))  

# DECILES 
def decile (row):
  if row['Good'] <= p_dec[1]:
    return 0
  ...
  if row['Good'] <= p_dec[9]:
    return 8
  if row['Good'] <= 20:
    return 9
  return -1

## Array of Deciles:
def decile_array(ml):
  list_to_np=[]
  for item in ml:
    list_to_np.append(item[0])
  p_np=np.array(list_to_np)
  p_dec=np.percentile(p_np,np.arrange(0,100,10))
  return(p_dec)
  print(p_dec)
  
  
  



  
    

  


